{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87VuLG5ghHQM"
   },
   "source": [
    "# Word2Vec\n",
    "\n",
    "In this notebook, you will implement and train two different flavors of word2vec models.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Complete the parts identified with `TODO: Implement`.  \n",
    "Your code should go between the `START CODE HERE` and `END CODE HERE` tags.\n",
    "\n",
    "⛔ **Note** You are NOT allowed to use external libraries.  \n",
    "You are NOT allowed to use autograd libraries like pytorch either.\n",
    "\n",
    "**Acknowledgement**: This assignment was designed with help of former TA of the course Jake Tae and further adapted to this semester by current course staff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9NRSOdvhHQN"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tLUufqvIhHQN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.5.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (1.26.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (4.67.3)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.7.1)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/natejly/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/natejly/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install\n",
    "%pip install datasets\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import Optional\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZKDmu3e-hHQO"
   },
   "outputs": [],
   "source": [
    "# some initializations\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aWeyXgq3hHQO"
   },
   "outputs": [],
   "source": [
    "# data loading. We use a small subset of OpenWebText\n",
    "def get_corpus(size: int = 1000) -> list[str]:\n",
    "    # NOTE: Loading from a specific revision where openwebtext has been updated\n",
    "    # to Parquet format since new versions of datasets don't allow scripts\n",
    "    dataset = load_dataset(\"stas/openwebtext-10k\", revision=\"refs/pr/3\")\n",
    "    corpus = dataset[\"train\"][\"text\"][:size]\n",
    "    return corpus\n",
    "\n",
    "\n",
    "# for testing purpuses we can also use a tiny corpus\n",
    "def get_tiny_corpus() -> list[str]:\n",
    "    document1 = \"Natural language processing is a subfield of computer science, linguistics, and machine learning.\"\n",
    "    document2 = \"It is concerned with giving computers the ability to support and manipulate natural language.\"\n",
    "    document3 = \"It involves processing natural language datasets using rule-based or probabilistic machine learning approaches.\"\n",
    "    document4 = \"The goal is a computer capable of understanding the contents of documents through machine learning.\"\n",
    "    corpus = [document1, document2, document3, document4]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xu4tkh2whHQO"
   },
   "source": [
    "## Part 1: Preprocessing and Data Preparation\n",
    "\n",
    "In this part, we will implement necessary functions to preprocess and load the data into our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "g-X3D7E6hHQO"
   },
   "outputs": [],
   "source": [
    "def preprocess(corpus: list[str]) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Applies:\n",
    "    1. lowercase transformation\n",
    "    2. leading & trailing whitespace removal\n",
    "    3. removing any non-alphanumeric symbols (e.g., punctuation)\n",
    "    4. splitting each documents by whitespace\n",
    "\n",
    "    Hint: You can (but don't have to) use regular expressions.\n",
    "    Also consider .strip() and .split() functions.\n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "\n",
    "    # TODO: Implement\n",
    "    for document in tqdm(corpus, desc=\"Preprocessing\"):\n",
    "        # START OF YOUR CODE\n",
    "        text = document.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = text.split()\n",
    "        result.append(text)\n",
    "        # END OF YOUR CODE\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "p544gc0_hHQO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 4/4 [00:00<00:00, 10638.69it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_vocabulary(corpus: list[str], vocab_size: int = 2000) -> list[str]:\n",
    "    \"\"\"\n",
    "    Tokenizes each document in the corpus and returns a list of distinct words.\n",
    "    Sort the words by most frequent to least frequent. If there are\n",
    "    more words than `vocab_size`, cut off the remaining infrequent words.\n",
    "    The output should not contain duplicate tokens.\n",
    "    You should also handle unseen tokens with an special <unk> token,\n",
    "    so make sure to include this special token in your vocabulary.\n",
    "\n",
    "    Hint: Consider using collections.Counter (already imported).\n",
    "    \"\"\"\n",
    "    corpus = preprocess(corpus)\n",
    "\n",
    "    # TODO: Implement (~3 lines)\n",
    "    # START OF YOUR CODE\n",
    "    word_counts = Counter(word for document in corpus for word in document)\n",
    "    word_counts.pop(\"<unk>\", None)\n",
    "    vocabulary = [\"<unk>\"] + [word for word, _ in word_counts.most_common(vocab_size-1)]\n",
    "    vocabulary = vocabulary[:vocab_size]\n",
    "\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "# sanity check\n",
    "vocab_size = 10\n",
    "test_vocabulary = get_vocabulary(get_tiny_corpus(), vocab_size=vocab_size)\n",
    "assert \"<unk>\" in test_vocabulary\n",
    "assert len(test_vocabulary) <= vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ONBUf_uOhHQO"
   },
   "outputs": [],
   "source": [
    "# helper function\n",
    "def get_vocab2idx(vocabulary: list[str]) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping vocabulary to its index (zero-indexed).\n",
    "    Example input/output shown below.\n",
    "\n",
    "    >>> get_vocab2idx(['a', 'b'])\n",
    "    {'a': 0, 'b': 1}\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement (~1 line)\n",
    "    # START OF YOUR CODE\n",
    "    vocab2idx = {}\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        vocab2idx[word] = i\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "    return vocab2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2GPVssr_hHQO"
   },
   "outputs": [],
   "source": [
    "def generate_training_data(\n",
    "    corpus: list[str], vocab2idx, window_size: int, k: int\n",
    ") -> list[tuple[int, list[int], list[int]]]:\n",
    "    \"\"\"\n",
    "    Generates the training data as a list. Each element of the list\n",
    "    follows the format:\n",
    "\n",
    "        (center word index, list of context word indicies, list of negative word indices)\n",
    "\n",
    "    The context word indices are are the indices of the words within the windows size inclusive.\n",
    "    The negative indicies are sampled from the entire vocabulary,\n",
    "    excluding the positive and context indices. To keep things simple, we use\n",
    "    uniform sampling. However, the original word2vec paper uses weighted sampling.\n",
    "\n",
    "    Hint: Consider using functions in the built-in random library,\n",
    "    such as random.choice(s).\n",
    "\n",
    "    Example output shown below (note that the numbers are not correct):\n",
    "    [\n",
    "        (10, [1, 2, 3], [0, 4, 5, 8, 9]),\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "    # TODO: Implement\n",
    "    # START OF YOUR CODE\n",
    "    tokenized_corpus = preprocess(corpus)\n",
    "    unk_idx = vocab2idx[\"<unk>\"]\n",
    "    vocab_indices = list(vocab2idx.values())\n",
    "\n",
    "    for tokens in tokenized_corpus:\n",
    "        token_indices = [vocab2idx.get(token, unk_idx) for token in tokens]\n",
    "        n_tokens = len(token_indices)\n",
    "\n",
    "        for center_pos, center_idx in enumerate(token_indices):\n",
    "            left = max(0, center_pos - window_size)\n",
    "            right = min(n_tokens, center_pos + window_size + 1)\n",
    "\n",
    "            context_indices = [\n",
    "                token_indices[pos] for pos in range(left, right) if pos != center_pos\n",
    "            ]\n",
    "            if not context_indices:\n",
    "                continue\n",
    "\n",
    "            blocked = set(context_indices)\n",
    "            blocked.add(center_idx)\n",
    "            candidates = [idx for idx in vocab_indices if idx not in blocked]\n",
    "\n",
    "            if not candidates:\n",
    "                negative_indices = []\n",
    "            elif k <= len(candidates):\n",
    "                negative_indices = random.sample(candidates, k)\n",
    "            else:\n",
    "                negative_indices = random.choices(candidates, k=k)\n",
    "\n",
    "            result.append((center_idx, context_indices, negative_indices))\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHwseOzZhHQP"
   },
   "source": [
    "## Part 2: Skipgram Model\n",
    "\n",
    "### Part 2.1: Activation Functions\n",
    "\n",
    "In this part,  we implement the activation functions (or non-linearities) that we need for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ELpOXc1QhHQP"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the sigmoid activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement (~1 line)\n",
    "    # START OF YOUR CODE\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the softmax activation function.\n",
    "    \"\"\"\n",
    "    # TODO: Implement (~1 line)\n",
    "    # START OF YOUR CODE\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "    # END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_r1pN4chHQP"
   },
   "source": [
    "### Part 2.2: Implementing the Skipgram model\n",
    "\n",
    "We implement two methods. The first is using just the softmax loss. The second is a more efficient implmenetation which uses negative sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "r2wE_g0ThHQP"
   },
   "outputs": [],
   "source": [
    "# Softmax loss and gradients function\n",
    "\n",
    "\n",
    "def get_softmax_loss_and_gradients(\n",
    "    v_c: np.ndarray,\n",
    "    u_idx: int,\n",
    "    U: np.ndarray,\n",
    "    negative_samples: Optional[list[int]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    This part implements the softmax loss and returns the gradients with respect to the input.\n",
    "\n",
    "    Given the center word v_c, the index of the context word u_idx,\n",
    "    and the word embedding matrix U, compute the softmax loss and\n",
    "    gradients for both the center word and the context word.\n",
    "\n",
    "    Args:\n",
    "      v_c: np.ndarray shape (dim)\n",
    "      u_idx: int\n",
    "      U: np.ndarray shape (V, dim)\n",
    "      negative_samples: Not used (ignore for this part)\n",
    "\n",
    "    Returns:\n",
    "      loss: float\n",
    "      grad_v_c: np.ndarray\n",
    "      grad_outside_vectors: np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement (~6 lines)\n",
    "\n",
    "    # START OF YOUR CODE\n",
    "    scores = U @ v_c\n",
    "    y_hat = softmax(scores)\n",
    "    loss = -np.log(y_hat[u_idx])\n",
    "\n",
    "    y_hat_minus_y = y_hat.copy()\n",
    "    y_hat_minus_y[u_idx] -= 1\n",
    "    grad_v_c = U.T @ y_hat_minus_y\n",
    "    grad_outside_vectors = np.outer(y_hat_minus_y, v_c)\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "    return loss, grad_v_c, grad_outside_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mj2nVQ8YhHQP"
   },
   "outputs": [],
   "source": [
    "# test your implementation on sample input with expected output\n",
    "\n",
    "v_c = np.asarray([-1.16867804, 1.14282281, 0.75193303])\n",
    "u_idx = 2\n",
    "U = np.asarray(\n",
    "    [\n",
    "        [-0.34271452, -0.80227727, -0.16128571],\n",
    "        [0.40405086, 1.8861859, 0.17457781],\n",
    "        [0.25755039, -0.07444592, -1.91877122],\n",
    "        [-0.02651388, 0.06023021, 2.46324211],\n",
    "        [-0.19236096, 0.30154734, -0.03471177],\n",
    "    ]\n",
    ")\n",
    "\n",
    "expected_loss = 4.575654879938905\n",
    "expected_grads_vc = np.asarray([-0.14065423, 0.84958569, 3.07103537])\n",
    "expected_grad_U = np.asarray(\n",
    "    [\n",
    "        [-0.03961545, 0.03873902, 0.02548877],\n",
    "        [-0.46011424, 0.44993491, 0.2960397],\n",
    "        [1.15664118, -1.13105225, -0.74418846],\n",
    "        [-0.52786724, 0.51618898, 0.33963231],\n",
    "        [-0.12904425, 0.12618934, 0.08302769],\n",
    "    ]\n",
    ")\n",
    "\n",
    "loss, grads_vc, grads_U = get_softmax_loss_and_gradients(v_c, u_idx, U)\n",
    "assert np.allclose(loss, expected_loss)\n",
    "assert np.allclose(grads_vc, expected_grads_vc)\n",
    "assert np.allclose(grads_U, expected_grad_U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "b716RCAVhHQP"
   },
   "outputs": [],
   "source": [
    "# negative sampling loss and corresponding gradients\n",
    "\n",
    "\n",
    "def get_negative_sampling_loss_and_gradients(\n",
    "    v_c: np.ndarray,\n",
    "    u_idx: int,\n",
    "    U: np.ndarray,\n",
    "    negative_samples_idx: list[int],\n",
    ") -> tuple[float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    This part implements the negative sampling loss and also returns the gradients.\n",
    "\n",
    "    Given the center word v_c, the index of the context word u_idx,\n",
    "    and the word embedding matrix U, compute the negative sampling loss and\n",
    "    gradients for both the center word and the context word.\n",
    "\n",
    "    Args:\n",
    "        v_c: np.ndarray shape (dim)\n",
    "        u_idx: int\n",
    "        v_c_idx: int, the index of the center word that we are considering\n",
    "          used to eliminate this from being selected as negative\n",
    "        U: np.ndarray shape (V, dim)\n",
    "        dataset: list[tuple[int, int]]\n",
    "        k: int, number of negative samples\n",
    "\n",
    "    Returns:\n",
    "        loss: float\n",
    "        grad_v_c: np.ndarray\n",
    "        grad_outside_vector: np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement (~8 lines)\n",
    "\n",
    "    # START OF YOUR CODE\n",
    "    grad_outside_vectors = np.zeros_like(U)\n",
    "\n",
    "    u_o = U[u_idx]\n",
    "    pos_score = np.dot(u_o, v_c)\n",
    "    pos_sigmoid = sigmoid(pos_score)\n",
    "    loss = -np.log(pos_sigmoid)\n",
    "    grad_v_c = (pos_sigmoid - 1) * u_o\n",
    "    grad_outside_vectors[u_idx] += (pos_sigmoid - 1) * v_c\n",
    "\n",
    "    for neg_idx in negative_samples_idx:\n",
    "        u_k = U[neg_idx]\n",
    "        neg_score = np.dot(u_k, v_c)\n",
    "        neg_sigmoid = sigmoid(neg_score)\n",
    "        loss += -np.log(sigmoid(-neg_score))\n",
    "        grad_v_c += neg_sigmoid * u_k\n",
    "        grad_outside_vectors[neg_idx] += neg_sigmoid * v_c\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "    return loss, grad_v_c, grad_outside_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "e9ONSAxUhHQP"
   },
   "outputs": [],
   "source": [
    "# test your implementation on sample input with expected output\n",
    "\n",
    "negative_samples_idx = [0, 3]\n",
    "\n",
    "expected_loss = 4.486897078520242\n",
    "expected_grads_vc = np.asarray([-0.36363528, -0.16053028, 3.75446939])\n",
    "expected_grad_U = np.asarray(\n",
    "    [\n",
    "        [-0.40411265, 0.39517227, 0.26000801],\n",
    "        [0.0, 0.0, 0.0],\n",
    "        [1.00696315, -0.98468561, -0.6478849],\n",
    "        [-1.02337143, 1.00073089, 0.65844207],\n",
    "        [0.0, 0.0, 0.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "loss, grads_vc, grads_U = get_negative_sampling_loss_and_gradients(\n",
    "    v_c, u_idx, U, negative_samples_idx\n",
    ")\n",
    "assert np.allclose(loss, expected_loss)\n",
    "assert np.allclose(grads_vc, expected_grads_vc)\n",
    "assert np.allclose(grads_U, expected_grad_U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeMgZy9ehHQP"
   },
   "source": [
    "## Part 3: Word2vec Class and Training\n",
    "\n",
    "In this part, we will implement a Word2vec class that actually trains the model.\n",
    "\n",
    "> **Note:**\n",
    "> We've done this part for you. You don't need to change anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "q5KzcAPOhHQP"
   },
   "outputs": [],
   "source": [
    "# now we implement the class that will train the model using our defined losses\n",
    "# DO NOT CHANGE THIS CLASS\n",
    "\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(\n",
    "        self,\n",
    "        corpus: list[str],\n",
    "        embedding_dim: int = 20,\n",
    "        learning_rate: float = 0.01,\n",
    "        num_negative_samples: int = 10,\n",
    "        window_size: int = 5,\n",
    "        loss_method: str = \"softmax\",  # or \"negative_sampling\"\n",
    "        save_freq: int = 1000,\n",
    "        cache_path: str = \"data/\",\n",
    "        log_freq: int = 1000,\n",
    "        vocab_size: int = 2000,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the model parameters and learning hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            corpus: list[str], list of documents\n",
    "            embedding_dim: int, dimension of the word embedding\n",
    "            learning_rate: float, learning rate for SGD\n",
    "            num_negative_samples: int, number of negative samples to use for negative sampling\n",
    "            window_size: int, window size for context words\n",
    "            loss_method: str, \"softmax\" or \"negative_sampling\"\n",
    "            save_freq: int, how often to save the model\n",
    "            cache_path: str, where to save the model\n",
    "            log_freq: int, how often to print the progress\n",
    "            vocab_size: int, size of the vocabulary\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        self.vocab = get_vocabulary(self.corpus, vocab_size=vocab_size)\n",
    "        self.vocab2idx = get_vocab2idx(self.vocab)\n",
    "        self.idx2vocab = {v: k for k, v in self.vocab2idx.items()}\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "        self.window_size = window_size\n",
    "        self.log_freq = log_freq\n",
    "        self.save_freq = save_freq\n",
    "\n",
    "        # the main parameters of the model that we are going to train\n",
    "        self.center_vecs = (np.random.rand(len(self.vocab), embedding_dim) - 0.5) / embedding_dim\n",
    "        self.outside_vecs = np.zeros((len(self.vocab), embedding_dim))\n",
    "\n",
    "        assert loss_method in [\"softmax\", \"negative_sampling\"]\n",
    "        if loss_method == \"softmax\":\n",
    "            self.loss_and_grad_fn = get_softmax_loss_and_gradients\n",
    "        else:\n",
    "            self.loss_and_grad_fn = get_negative_sampling_loss_and_gradients\n",
    "\n",
    "        # cache the training data\n",
    "        self.cache_path = cache_path\n",
    "        data_cache_path = f\"{cache_path}/data.npy\"\n",
    "        if not os.path.exists(data_cache_path):\n",
    "            pathlib.Path(cache_path).mkdir(parents=True, exist_ok=True)\n",
    "            self.data = generate_training_data(\n",
    "                corpus, self.vocab2idx, window_size, num_negative_samples\n",
    "            )\n",
    "            # save mmap\n",
    "            with open(data_cache_path, \"wb\") as f:\n",
    "                pickle.dump(self.data, f)\n",
    "        else:\n",
    "            # load from mmap\n",
    "            with open(data_cache_path, \"rb\") as f:\n",
    "                self.data = pickle.load(f)\n",
    "\n",
    "    def train_step(\n",
    "        self,\n",
    "        center_word_idx: int,\n",
    "        outside_words_indexes: list[int],\n",
    "        negative_idxs: Optional[list[int]] = None,  # only used for negative sampling\n",
    "    ) -> tuple[float, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Run the train step for a given center word, outside (context) words, and negative samples (if applicable)\"\"\"\n",
    "\n",
    "        loss = 0.0\n",
    "        grad_center_vectors = np.zeros(self.center_vecs.shape)\n",
    "        gradoutside_vectors = np.zeros(self.outside_vecs.shape)\n",
    "\n",
    "        for ow_idx in outside_words_indexes:\n",
    "            center_word_idx = center_word_idx\n",
    "            loss_j, grad_v_c, grad_outside_vector_j = self.loss_and_grad_fn(\n",
    "                self.center_vecs[center_word_idx],\n",
    "                ow_idx,\n",
    "                self.outside_vecs,\n",
    "                negative_idxs,\n",
    "            )\n",
    "            loss += loss_j\n",
    "            grad_center_vectors[center_word_idx] += grad_v_c\n",
    "            gradoutside_vectors += grad_outside_vector_j\n",
    "\n",
    "        return loss, grad_center_vectors, gradoutside_vectors\n",
    "\n",
    "    def save(self, current_step: int) -> None:\n",
    "        np.save(f\"{self.cache_path}/center_vecs_{current_step}.npy\", self.center_vecs)\n",
    "        np.save(f\"{self.cache_path}/outside_vecs_{current_step}.npy\", self.outside_vecs)\n",
    "\n",
    "    def load(self, checkpoint_path: str, step: int) -> None:\n",
    "        # load the latest checkpoint\n",
    "        self.center_vecs = np.load(f\"{checkpoint_path}/center_vecs_{step}.npy\")\n",
    "        self.outside_vecs = np.load(f\"{checkpoint_path}/outside_vecs_{step}.npy\")\n",
    "\n",
    "    def get_embeddings_avg(self) -> np.ndarray:\n",
    "        return (self.center_vecs + self.outside_vecs) / 2\n",
    "\n",
    "    def get_embeddigns_concat(self) -> np.ndarray:\n",
    "        return np.concatenate((self.center_vecs, self.outside_vecs), axis=1)\n",
    "\n",
    "    def train(\n",
    "        self, batch_size: int = 32, num_epochs: int = 10, num_steps: Optional[int] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        The training loop of the model\n",
    "        Batch size is simulated by gradient accumulation (code doesn't support batch dimension)\n",
    "\n",
    "        \"\"\"\n",
    "        gradient_accumulation_steps = batch_size\n",
    "\n",
    "        global_steps = 0\n",
    "        steps = 0\n",
    "        loss = 0.0\n",
    "        grad_center = np.zeros(self.center_vecs.shape)\n",
    "        grad_outside = np.zeros(self.outside_vecs.shape)\n",
    "        total_batches = len(self.data) // batch_size\n",
    "        if num_steps is not None:\n",
    "            stop_at = num_steps\n",
    "            num_epochs = 10000  # not used anymore\n",
    "        else:\n",
    "            stop_at = total_batches * num_epochs\n",
    "        done = False\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            if done:\n",
    "                break\n",
    "            local_step = 0\n",
    "            for center_idx, outside_word_indexes, negative_idxs in self.data:\n",
    "                current_loss, center_grad, outside_grad = self.train_step(\n",
    "                    center_idx,\n",
    "                    outside_word_indexes,\n",
    "                    negative_idxs,\n",
    "                )\n",
    "                if steps % gradient_accumulation_steps == 0:\n",
    "                    grad_center += center_grad\n",
    "                    grad_center /= batch_size\n",
    "                    self.center_vecs -= self.learning_rate * grad_center\n",
    "\n",
    "                    grad_outside += outside_grad\n",
    "                    grad_outside /= batch_size\n",
    "                    self.outside_vecs -= self.learning_rate * grad_outside\n",
    "\n",
    "                    # zero out the gradients\n",
    "                    grad_center = np.zeros(self.center_vecs.shape)\n",
    "                    grad_outside = np.zeros(self.outside_vecs.shape)\n",
    "                    global_steps += 1\n",
    "\n",
    "                    if global_steps % self.log_freq == 0:\n",
    "                        progress_percent = round(global_steps / stop_at * 100, 2)\n",
    "                        print(\n",
    "                            f\"ep {epoch} step {local_step} global step {global_steps} n_batches {total_batches} progress {progress_percent}%: loss {(loss / steps):.3f}\"\n",
    "                        )\n",
    "\n",
    "                    if global_steps % self.save_freq == 0:\n",
    "                        self.save(global_steps)\n",
    "\n",
    "                loss += current_loss\n",
    "                steps += 1\n",
    "                local_step += 1\n",
    "\n",
    "                if global_steps >= stop_at:\n",
    "                    done = True\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nIWV5ZyThHQP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 100/100 [00:00<00:00, 9025.44it/s]\n",
      "Preprocessing: 100%|██████████| 100/100 [00:00<00:00, 10883.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0 step 63936 global step 1000 n_batches 1167 progress 85.69%: loss 34.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 100/100 [00:00<00:00, 5579.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0 step 63936 global step 1000 n_batches 1167 progress 85.69%: loss 30.442\n",
      "Nearest neighbors for softmax model\n",
      "<unk> [('<unk>', 0.0), ('were', 0.058117743106800936), ('dobson', 0.058529777830171145), ('liga', 0.05955627904550008), ('dinner', 0.060636233692665076)]\n",
      "said [('said', 0.0), ('jail', 0.022853871360644512), ('bristol', 0.025014559215132293), ('shriberg', 0.025257632039757946), ('38', 0.02624368798740035)]\n",
      "said [('said', 0.0), ('jail', 0.022853871360644512), ('bristol', 0.025014559215132293), ('shriberg', 0.025257632039757946), ('38', 0.02624368798740035)]\n",
      "\n",
      "---\n",
      "Nearest neighbors for negative sampling model\n",
      "<unk> [('<unk>', 0.0), ('north', 0.0265327013088875), ('online', 0.028876199263734546), ('maximum', 0.030212628992952673), ('hell', 0.030900694951005492)]\n",
      "said [('said', 0.0), ('waste', 0.022140011124549486), ('losing', 0.02375417395954198), ('reasonably', 0.02444103867124599), ('fan', 0.02462374798086869)]\n",
      "said [('said', 0.0), ('waste', 0.022140011124549486), ('losing', 0.02375417395954198), ('reasonably', 0.02444103867124599), ('fan', 0.02462374798086869)]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "# Setting the hyperparameters\n",
    "\n",
    "embedding_dim = 20\n",
    "learning_rate = 0.7\n",
    "training_method = \"softmax\"\n",
    "num_negative_samples = 10\n",
    "window_size = 2\n",
    "batch_size = 64\n",
    "num_epochs = 1\n",
    "save_freq = 1000000\n",
    "cache_path = \"data\"\n",
    "vocab_size = 5000\n",
    "limit_data_size = 100\n",
    "\n",
    "corpus = get_corpus(size=limit_data_size)\n",
    "\n",
    "\n",
    "def get_model(training_method: str = \"softmax\") -> Word2Vec:\n",
    "    word2vec = Word2Vec(\n",
    "        corpus,\n",
    "        embedding_dim=embedding_dim,\n",
    "        learning_rate=learning_rate,\n",
    "        loss_method=training_method,\n",
    "        num_negative_samples=num_negative_samples,\n",
    "        window_size=window_size,\n",
    "        save_freq=save_freq,\n",
    "        vocab_size=vocab_size,\n",
    "        cache_path=cache_path,\n",
    "    )\n",
    "    word2vec.train(batch_size=batch_size, num_epochs=num_epochs)\n",
    "    return word2vec\n",
    "\n",
    "\n",
    "word2vec_softmax = get_model(training_method=\"softmax\")\n",
    "word2vec_negative_sampling = get_model(training_method=\"negative_sampling\")\n",
    "word2vec_softmax.save(\"final-softmax\")\n",
    "word2vec_negative_sampling.save(\"final-negative_sampling\")\n",
    "\n",
    "\n",
    "def load_stopwords() -> set[str]:\n",
    "    import nltk\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    stopwords = set(stopwords)\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "\n",
    "def get_nearest_neighbors(\n",
    "    word: str, embeddings: np.ndarray, vocab2idx: dict[str, int], idx2vocab: list[str], k: int = 5\n",
    ") -> list[tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Get the k nearest neighbors for a given word.\n",
    "    \"\"\"\n",
    "    idx = vocab2idx[word]\n",
    "    embedding = embeddings[idx]\n",
    "    distances = np.linalg.norm(embedding - embeddings, axis=1)\n",
    "    sorted_distances = np.argsort(distances)\n",
    "    return [(idx2vocab[idx], distances[idx]) for idx in sorted_distances[:k]]\n",
    "\n",
    "\n",
    "def print_nearest_neighbors(model: Word2Vec, k: int = 5, num_print: int = 20) -> None:\n",
    "    # print nearest neighbors\n",
    "    j = 0\n",
    "    for word in model.vocab:\n",
    "        j += 1\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        print(\n",
    "            word,\n",
    "            get_nearest_neighbors(\n",
    "                word,\n",
    "                model.get_embeddings_avg(),\n",
    "                model.vocab2idx,\n",
    "                model.idx2vocab,\n",
    "            ),\n",
    "        )\n",
    "        if j > num_print:\n",
    "            break\n",
    "    print(\n",
    "        word,\n",
    "        get_nearest_neighbors(\n",
    "            word, model.get_embeddings_avg(), model.vocab2idx, model.idx2vocab, k=k\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Nearest neighbors for softmax model\")\n",
    "print_nearest_neighbors(word2vec_softmax)\n",
    "print(\"\\n---\\nNearest neighbors for negative sampling model\")\n",
    "print_nearest_neighbors(word2vec_negative_sampling)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1f9RIb0nPQ1RMaibeiIFVqmA1M8bft2_N",
     "timestamp": 1768950972672
    },
    {
     "file_id": "11PSsPWg-xCeQagxubkyFEdllPuXh3Jm6",
     "timestamp": 1737731172118
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
