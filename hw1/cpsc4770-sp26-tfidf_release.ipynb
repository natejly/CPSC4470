{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4G--JVIfw2E"
   },
   "source": [
    "# Sparse Word Representations Assignment (50 points)\n",
    "\n",
    "**Objective:** Implement and apply sparse word representations using co-occurrence counts and TF-IDF weighting on a small document collection.\n",
    "\n",
    "**Dataset:** We will use the 20 Newsgroups dataset, readily available in scikit-learn. We'll limit it to a subset of categories and a reduced number of documents to keep things manageable.\n",
    "\n",
    "```\n",
    "Instructions:\n",
    "Make a copy of this code notebook into your own account.\n",
    "Fill out the parts that are specified with\n",
    "% -- Your Implementation -- %\n",
    "Then submit your completed notebook with all the outputs to Gradescope.\n",
    "```\n",
    "\n",
    "### 1. Data Loading and Preprocessing (5 points)\n",
    "\n",
    "Steps:\n",
    "- Load the 20 Newsgroups dataset using scikit-learn, but only select 4 categories of your choice.\n",
    "- Limit the dataset to the first 1000 documents.\n",
    "- Tokenize the documents into words. You can use a simple tokenizer like splitting on spaces and remove any punctuations.\n",
    "    - For this step you are welcome to use python's built in functions/libraries such as `re`, `str`, `string.punctuation`, etc. But you are not allowed to use more specialized functions such as `nltk` or `spacy`.\n",
    "    - Regarding punctuation, there is some flexibility in this and you are welcome to make some design decisions where needed (e.g., whether to split on apostrophes). Please specify any design decisions or assumptions you make in your submission.\n",
    "- Create a vocabulary of the 5000 most frequent words across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OLFntWSffw2F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "%pip install scikit-learn\n",
    "from typing import List, Set, Dict, Tuple, Union\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from math import log\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ljp7y8zkfw2G"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load the dataset with selected categories\n",
    "categories = ['talk.politics.misc', 'sci.med']\n",
    "newsgroups_data = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ccu947tQfw2G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5000\n",
      "\n",
      "Sample vocabulary words:\n",
      "['tend', 'tour', 'friends', 'yeah', 'abortion', 'dynamics', 'minute', 'mass', 'cornea', 'game']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Set\n",
    "import heapq\n",
    "\n",
    "def preprocess_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize and clean text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing punctuation\n",
    "    3. Splitting on whitespace\n",
    "    4. Removing empty tokens\n",
    "    \"\"\"\n",
    "    #\n",
    "    # % -- Your Implementation -- %\n",
    "    #\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "# Get first 1000 documents\n",
    "documents = newsgroups_data.data[:1000]\n",
    "\n",
    "# Tokenize all documents\n",
    "tokenized_docs = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "# Now write a fucntion to get the vocabulary\n",
    "def get_vocab(tokenized_docs: List[List[str]], vocab_size: int) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Create vocabulary from tokenized documents by:\n",
    "    1. Counting word frequencies across all documents\n",
    "    2. Taking the `vocab_size` most common words\n",
    "\n",
    "    Args:\n",
    "        tokenized_docs: List of documents, where each document is a list of tokens\n",
    "\n",
    "    Returns:\n",
    "        Set of vocabulary words\n",
    "    \"\"\"\n",
    "    # storing tuples (frequency, word)\n",
    "    freqs = {}\n",
    "    for doc in tokenized_docs:\n",
    "        for word in doc:\n",
    "            if word not in freqs:   \n",
    "                freqs[word] = 0\n",
    "            freqs[word] += 1\n",
    "    sorted_freqs = sorted(freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "    return set([word for word, freq in sorted_freqs[:vocab_size]])  \n",
    "\n",
    "\n",
    "\n",
    "vocab = get_vocab(tokenized_docs, vocab_size=5000)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(\"\\nSample vocabulary words:\")\n",
    "print(list(vocab)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hL-VE_ckfw2G"
   },
   "source": [
    "## 2. Computing the co-occurrence matrix (10 points)\n",
    "\n",
    "Assume co-occurrence means if a word appears in the same document and within a window of size 10 of another word. Compute the co-occurrence matrix based on raw frequencies.\n",
    "\n",
    "For example in the sentence: \"the hungry cat ate the small fish near the blue lake\"\n",
    "\n",
    "With window size 4, the co-occurrences for the word \"cat\":\n",
    "\n",
    "- Left context (within 2 words): \"the\", \"hungry\"\n",
    "\n",
    "- Right context (within 1 words): \"ate\"\n",
    "\n",
    "- So \"cat\" co-occurs once each with: the, hungry, ate\n",
    "\n",
    "Words outside the window like \"fish\", \"near\", \"blue\", \"lake\" are not counted as co-occurrences with \"cat\" in this case.\n",
    "\n",
    "Complete the following steps:\n",
    "\n",
    "- Construct a co-occurrence matrix. This matrix will have dimensions `vocab_size` x `vocab_size` (5000 x 5000 in our case).\n",
    "- For each document, iterate through its words. For each word, increment the corresponding entries in the co-occurrence matrix for its neighboring words within a specified window size (e.g., a window of 5 words to the left and right).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pzwjpcGIfw2G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence matrix shape: (5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_cooccurrence_matrix(tokenized_docs: List[List[str]], vocab: Set[str], window_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute word co-occurrence matrix from tokenized documents using a sliding window approach.\n",
    "\n",
    "    Args:\n",
    "        tokenized_docs (List[List[str]]): List of documents where each document is a list of tokens\n",
    "        vocab (Set[str]): Set of vocabulary words to consider\n",
    "        window_size (int): Number of words to consider\n",
    "            if odd, there are window_size//2 words on each side of the center word\n",
    "            if even, there are window_size//2 words on the left and window_size//2 - 1 words on the right\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Co-occurrence matrix of shape (vocab_size, vocab_size) where entry [i,j]\n",
    "                   represents how many times word i appears within the window of word j\n",
    "    \"\"\"\n",
    "    #\n",
    "    # % -- Your Implementation -- %\n",
    "    #\n",
    "    word_to_idx = {word: i for i, word in enumerate(sorted(vocab))}\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    cooccurrence_matrix = np.zeros((vocab_size, vocab_size))\n",
    "    \n",
    "    for doc in tokenized_docs:\n",
    "        for i, word in enumerate(doc):\n",
    "            if word not in vocab:\n",
    "                continue\n",
    "            \n",
    "            if window_size % 2 == 0:\n",
    "                left = i - window_size // 2\n",
    "                right = i + window_size // 2 \n",
    "            else:  # odd\n",
    "                left = i - window_size // 2\n",
    "                right = i + window_size // 2 + 1\n",
    "            \n",
    "            for j in range(max(0, left), min(len(doc), right)): # ensure that we are not out of bounds\n",
    "                if i != j and doc[j] in vocab:\n",
    "                    cooccurrence_matrix[word_to_idx[word], word_to_idx[doc[j]]] += 1\n",
    "    \n",
    "    return cooccurrence_matrix\n",
    "\n",
    "\n",
    "cooccurrence_matrix = compute_cooccurrence_matrix(tokenized_docs, vocab, window_size=10)\n",
    "print(f\"Co-occurrence matrix shape: {cooccurrence_matrix.shape}\")\n",
    "\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    #\n",
    "    # % -- Your Implementation -- %\n",
    "    #\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWHWBesPfw2G"
   },
   "source": [
    "## 3. Application: Word Similarity (10 points)\n",
    "\n",
    "- Implement a function to calculate the cosine similarity between two *word* vectors.\n",
    "- For the following words, find the 5 most similar words to each of your chosen words based on their raw co-occurrence representations using the cosine similarity function.\n",
    "[concerned, lone, matthew]\n",
    "- Print the chosen words and their 5 most similar words, along with their similarity scores.\n",
    "\n",
    "Recall that `Cosine Similarity(A, B) = (A . B) / (||A|| * ||B||)`\n",
    "where `.` denotes the dot product, and `||A||` is the magnitude (Euclidean norm) of vector A.\n",
    "\n",
    "Note: You may pass `vocab` into `find_most_similar_words()` if desired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "29ipOjDofw2G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('being', 0.7846449225081631), ('what', 0.7793101401464834), ('this', 0.7709015495877185), ('that', 0.7676644685283173), ('as', 0.7665753424810565), ('scientist', 0.6320018010533753), ('few', 0.6301430726198638), ('physician', 0.623335714251067), ('bit', 0.6146013164009455), ('chance', 0.5840981798624632), ('deane', 0.6555213366563067), ('deanebinahccbrandeisedu', 0.6313641498019763), ('david', 0.3519381814343646), ('wright', 0.29777500019127906), ('dn', 0.2440231932656964)]\n"
     ]
    }
   ],
   "source": [
    "words = ['concerned', 'lone', 'matthew']\n",
    "\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    #\n",
    "    # % -- Your Implementation -- %\n",
    "    #\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "\n",
    "def find_most_similar_words(words: List[str], cooccurrence_matrix: np.ndarray) -> List[Tuple[str, List[Tuple[str, float]]]]:\n",
    "    \"\"\"\n",
    "    Find the 5 most similar words to each input word based on their co-occurrence representations.\n",
    "\n",
    "    Args:\n",
    "        words (List[str]): List of words to find similar words for\n",
    "        cooccurrence_matrix (np.ndarray)\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, List[Tuple[str, float]]]]: For each input word, returns a tuple containing:\n",
    "            - The input word\n",
    "            - A list of 5 tuples, each containing:\n",
    "                - A similar word\n",
    "                - The cosine similarity score between the input and similar word\n",
    "\n",
    "    Example:\n",
    "        >>> result = find_most_similar_words(['cat'], cooccurrence_matrix)\n",
    "        >>> result\n",
    "        [('cat', [('dog', 0.85), ('kitten', 0.76), ('pet', 0.72), ...])]\n",
    "    \"\"\"\n",
    "    #\n",
    "    # % -- Your Implementation -- %\n",
    "    #\n",
    "    \n",
    "    word_to_idx = {word: i for i, word in enumerate(sorted(vocab))}\n",
    "    idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in vocab:\n",
    "            results.append((word, []))\n",
    "            continue\n",
    "        \n",
    "        word_idx = word_to_idx[word]\n",
    "        word_vec = cooccurrence_matrix[word_idx]\n",
    "        \n",
    "        similarities = []\n",
    "        for i in range(len(vocab)):\n",
    "            if i == word_idx:\n",
    "                continue\n",
    "            other_vec = cooccurrence_matrix[i]\n",
    "            sim = cosine_similarity(word_vec, other_vec)\n",
    "            similarities.append((sim,idx_to_word[i]))\n",
    "        \n",
    "        similarities.sort(reverse=True)\n",
    "        top_5 = similarities[:5]\n",
    "        for tup in top_5:\n",
    "            results.append((tup[1],tup[0]))\n",
    "            \n",
    "    return results\n",
    "\n",
    "print(find_most_similar_words(words, cooccurrence_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcGhQ9yGfw2G"
   },
   "source": [
    "## 4. TF-IDF weighting (10 points)\n",
    "\n",
    "In this part you will implement TF-IDF weighting, which is a numerical statistic that reflects how important a word is to a document in a collection or corpus. TF-IDF consists of two components:\n",
    "1. Term Frequency (TF): How frequently a word appears in a document  \n",
    "2. Inverse Document Frequency (IDF): How unique or rare the word is across all documents  \n",
    "\n",
    "Assume we have a term-document matrix of co-occurrences.\n",
    "Then we can use TF-IDF to reweight the raw frequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXKfPPlIfw2G"
   },
   "source": [
    "First compute the TF. Recall that TF is:\n",
    "\n",
    "tf(t,d) = 1 + log(count(t,d)) if term t appears in document d  \n",
    "tf(t,d) = 0 if term t doesn't appear in document d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pj2Oz8unfw2H"
   },
   "outputs": [],
   "source": [
    "def compute_tf(document: Union[str, List[str]], vocab: Set[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute term frequencies in a document.\n",
    "\n",
    "    \"\"\"\n",
    "    #\n",
    "    # % -- Your Implementation -- %\n",
    "    #\n",
    "    word_counts = {}\n",
    "    for word in document:\n",
    "        if word in vocab:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    \n",
    "    # Compute TF for each word in vocab\n",
    "    tf_scores = {}\n",
    "    for word in vocab:\n",
    "        if word in word_counts:\n",
    "            tf_scores[word] = 1 + np.log(word_counts[word])\n",
    "        else:\n",
    "            tf_scores[word] = 0.0\n",
    "    \n",
    "    return tf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkUsbBXDfw2H"
   },
   "source": [
    "### Compute inverse document frequency\n",
    "Implement a function to calculate document frequencies across a corpus.\n",
    "Recall that inverse document frequency is:\n",
    "\n",
    "log ( N / df ), where:\n",
    "\n",
    "- **N** is the total number of documents in the corpus.\n",
    "- **df** is the document frequency, i.e., the number of documents in which the term appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MrYoAYRGfw2H"
   },
   "outputs": [],
   "source": [
    "def compute_idf(documents: Union[List[str], List[List[str]]], vocab: Set[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute inverse document frequency for all terms in the corpus.\n",
    "    Returns a dictionary mapping terms to their IDF scores.\n",
    "    \"\"\"\n",
    "    #\n",
    "    # % -- Your Implementation -- %\n",
    "    #\n",
    "    N = len(documents)\n",
    "    \n",
    "    doc_freq = {}\n",
    "    for word in vocab:\n",
    "        doc_freq[word] = 0\n",
    "    \n",
    "    for doc in documents:\n",
    "        unique_words = set(doc)\n",
    "        for word in unique_words:\n",
    "            if word in vocab:\n",
    "                doc_freq[word] += 1\n",
    "    \n",
    "    idf_scores = {}\n",
    "    for word in vocab:\n",
    "        if doc_freq[word] > 0:\n",
    "            idf_scores[word] = np.log(N / doc_freq[word])\n",
    "    return idf_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjFC17B-fw2H"
   },
   "source": [
    "### Caclculate the co-occurrence matrix with TF-IDF weights.\n",
    "\n",
    "Now we want to calculate TF-IDF scores for each term in each document. We'll combine the term frequency (TF) and inverse document frequency (IDF) scores we computed earlier to get the final TF-IDF weights.  \n",
    "This will give us a measure of how important each word is to a document in the context of the entire corpus.\n",
    "\n",
    "Recall that the TF-IDF score is calculated as:  \n",
    "`tf-idf = tf * idf`  \n",
    "\n",
    "Where:  \n",
    "- tf is the term frequency (number of times term appears in document)  \n",
    "- idf is the inverse document frequency `(log(N/df))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FUGwrQ25fw2H"
   },
   "outputs": [],
   "source": [
    "def compute_tfidf(tokenized_docs: List[List[str]], vocab: Set[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute TF-IDF (Term Frequency-Inverse Document Frequency) matrix for a collection of documents.\n",
    "\n",
    "    The TF-IDF score is calculated as:\n",
    "    tf-idf(t,d) = tf(t,d) * idf(t)\n",
    "\n",
    "    Args:\n",
    "        tokenized_docs (List[List[str]]): List of documents, where each document is a list of tokens\n",
    "        vocab (Set[str]): Set of vocabulary words to consider\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: TF-IDF matrix of shape (vocab_size, num_documents) where:\n",
    "            - Each row represents a term from the vocabulary\n",
    "            - Each column represents a document\n",
    "            - Entry [i,j] represents the TF-IDF score of term i in document j\n",
    "\n",
    "    Note:\n",
    "        - IDF calculation includes +1 smoothing to avoid division by zero\n",
    "        - TF calculation uses log normalization: 1 + log(term_count)\n",
    "    \"\"\"\n",
    "    #\n",
    "    # % -- Your Implementation -- %\n",
    "    #\n",
    "    vocab_size = len(vocab)\n",
    "    num_docs = len(tokenized_docs)\n",
    "    \n",
    "    word_to_idx = {word: i for i, word in enumerate(sorted(vocab))}\n",
    "    \n",
    "    idf_scores = compute_idf(tokenized_docs, vocab)\n",
    "    \n",
    "    tfidf_matrix = np.zeros((vocab_size, num_docs))\n",
    "    \n",
    "    for doc_idx, doc in enumerate(tokenized_docs):\n",
    "        tf_scores = compute_tf(doc, vocab)\n",
    "        \n",
    "        for word in vocab:\n",
    "            word_idx = word_to_idx[word]\n",
    "            tfidf_matrix[word_idx, doc_idx] = tf_scores[word] * idf_scores[word]\n",
    "    \n",
    "    return tfidf_matrix\n",
    "word_document_matrix = compute_tfidf(tokenized_docs, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prjtnDpkfw2H"
   },
   "source": [
    "### Application in search (10 points)\n",
    "\n",
    "Now we will use TF-IDF representations to implement a simple search function.\n",
    "In this function we match the most similar documents to a given query.\n",
    "The query itself should be converted to a TF-IDF vector as above.\n",
    "\n",
    "Implement the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sWDVvyjEfw2H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most relevant documents for query: 'medical treatment for heart disease'\n",
      "\n",
      "Document 843 (score: 0.228):\n",
      "herman i would think you of all people wouldcould distinguish between health and treatment of diseas...\n",
      "\n",
      "Document 894 (score: 0.191):\n",
      "turkish president turgur ozal has passed away today after a heart attack in ankara at 1100 am gmt mr...\n",
      "\n",
      "Document 73 (score: 0.189):\n",
      "stimulation of the vagus nerve slows the heart and drops the blood pressure gordon banks n3jxp skept...\n",
      "\n",
      "Document 44 (score: 0.185):\n",
      "the term arrhythmia is usually used to encompass a wide range of abnormal heart rhythms cardiac dysr...\n",
      "\n",
      "Document 441 (score: 0.157):\n",
      "the funny thing is the personaly stories about reactions to msg vary so greatly some said that their...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search_documents(query: str, tokenized_docs: List[List[str]], tfidf_matrix: np.ndarray, vocab: Set[str]) -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Search for documents most relevant to a query using TF-IDF representations.\n",
    "\n",
    "    Args:\n",
    "        query (str): Search query string\n",
    "        tokenized_docs (List[List[str]]): List of documents where each document is a list of tokens\n",
    "        tfidf_matrix (np.ndarray): TF-IDF matrix of shape (vocab_size, num_documents)\n",
    "        vocab (Set[str]): Vocabulary set used for TF-IDF computation\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[int, float]]: List of (document_index, similarity_score) tuples,\n",
    "            sorted by relevance (highest similarity first)\n",
    "    \"\"\"\n",
    "    #\n",
    "    # % -- Your Implementation -- %\n",
    "    #\n",
    "    word_to_idx = {word: i for i, word in enumerate(sorted(vocab))}\n",
    "    query_tokens = preprocess_text(query)\n",
    "    query_tf = compute_tf(query_tokens, vocab)\n",
    "    idf_scores = compute_idf(tokenized_docs, vocab)\n",
    "\n",
    "    query_vec = np.zeros(len(vocab))\n",
    "    for word in vocab:\n",
    "        query_vec[word_to_idx[word]] = query_tf[word] * idf_scores.get(word, 0)\n",
    "\n",
    "    query_norm = np.linalg.norm(query_vec)\n",
    "    results = []\n",
    "    for doc_idx in range(tfidf_matrix.shape[1]):\n",
    "        doc_vec = tfidf_matrix[:, doc_idx]\n",
    "        denom = query_norm * np.linalg.norm(doc_vec)\n",
    "        if denom > 0:\n",
    "            score = float(np.dot(query_vec, doc_vec) / denom)\n",
    "        else:\n",
    "            score = 0\n",
    "        results.append((doc_idx, score))\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "query = \"medical treatment for heart disease\"\n",
    "results = search_documents(query, tokenized_docs, word_document_matrix, vocab)\n",
    "\n",
    "print(f\"Top 5 most relevant documents for query: '{query}'\\n\")\n",
    "for doc_idx, score in results[:5]:\n",
    "    # Print first 100 characters of each document\n",
    "    preview = ' '.join(tokenized_docs[doc_idx])[:100] + \"...\"\n",
    "    print(f\"Document {doc_idx} (score: {score:.3f}):\")\n",
    "    print(f\"{preview}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOBuomGBfw2H"
   },
   "source": [
    "## 5. Reflection questions (10 points)\n",
    "\n",
    "Now please answer the following reflection questions:\n",
    "\n",
    "1- How do the most similar words differ when using raw co-occurrence counts versus TF-IDF weighted representations? What might explain these differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zUO6l5cfw2H"
   },
   "source": [
    "# Raw co-occurance rewards frequency and local proximity where TF-IDF brings out the words that are distinctive to fewer documents. Thus Raw co-occurnace will be more likley to return many frequent and generic neighbors where TF-IDF is better at returning more content related terms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcM1gm7lfw2H"
   },
   "source": [
    "2. What are the limitations of using a fixed window size for co-occurrence counting? How might this impact the quality of word similarities for different types of words (e.g., nouns vs. verbs)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaPP9c1Tfw2H"
   },
   "source": [
    "# A fixed window size may not be optimal because if it is too small we can miss out on long range semantics but if it is too large it is xlumsy and can add noise and unrelated words. Nouns may benefit from nearby descriptive modifiers where verbs will depend on more long range structure, so a fixed window may weigh these classes in a biased manner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwGCorCyfw2H"
   },
   "source": [
    "3- How does the choice of preprocessing steps (like removing punctuation, converting to lowercase, etc.) affect the resulting word representations? Can you think of cases where preserving some of this information might be beneficial?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XhIj4gDfw2H"
   },
   "source": [
    "# Yes, for example Apple vs apple mean a technology company versus a food, or U.S vs us, or even removing ! and ? remove some semantics from sentances that signifigantly alter meaning. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
